{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Policy Gradients\n",
    "\n",
    "**Before doing this, be sure you review the `README.md` file from the homework!**\n",
    "\n",
    "\n",
    "\n",
    "You will implement the vanilla policy gradients algorithm, also referred to as\n",
    "REINFORCE.\n",
    "\n",
    "## Review\n",
    "\n",
    "In policy gradients, the objective is to learn a parameter $\\theta^*$ that\n",
    "maximizes the following objective:\n",
    "\n",
    "\\begin{equation}\\label{eq:pg}\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta(\\tau)}[R(\\tau)]\n",
    "\\end{equation}\n",
    "\n",
    "where $\\tau = (s_1,a_1,s_2,\\ldots,s_{T-1},a_{T-1},s_T)$ is a *trajectory*\n",
    "(also referred to as an *episode*), and factorizes as\n",
    "\n",
    "\\begin{equation}\\label{eq:factorization}\n",
    "\\pi_\\theta(\\tau) = p(s_1)\\pi_\\theta(a_1|s_1)\\prod_{t=2}^{T} p(s_t|s_{t-1},a_{t-1})\\pi_\\theta(a_t|s_t)\n",
    "\\end{equation}\n",
    "\n",
    "and $R(\\tau)$ denotes the full trajectory reward $R(\\tau) = \\sum_{t=1}^{T}\n",
    "r(s_t,a_t)$ with $r(s_t,a_t)$ the rewards at the individual time steps.\n",
    "\n",
    "In policy gradients, we directly apply the gradient $\\nabla_\\theta$ to\n",
    "$J(\\theta)$. In order to do so, we require samples of trajectories, meaning that\n",
    "we now denote them as $\\tau_i$ for the $i$th trajectory, and have $\\tau_i =\n",
    "(s_{i1},a_{i1},s_{i2},\\ldots,s_{iT})$. When we approximate the gradient with\n",
    "samples, we get:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) &\\approx \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\log \\pi_\\theta(\\tau_i) R(\\tau_i) \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^N \\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right)  \\left( \\sum_{t=1}^{T} r(s_{it},a_{it}) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Multiplying a discount factor $\\gamma$ to the rewards can be interpreted as\n",
    "encouraging the agent to focus on rewards closer in the future, which can also\n",
    "be thought of as a means for reducing variance (because there are more\n",
    "possible futures further into the future). The discount factor can be\n",
    "incorporated in two ways, from the full trajectory:\n",
    "\n",
    "\\begin{equation}\\label{eq:full_traj}\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
    "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
    "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "and from the reward to go:\n",
    "\n",
    "\\begin{equation}\\label{eq:to_go}\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
    "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
    "\\left( \\sum_{t'=t}^T \\gamma^{t'-t} r(s_{it},a_{it}) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "You will implement both versions in this assignment.\n",
    "\n",
    "*Remark*: It's possible to reduce variance by subtracting a (learned) baseline\n",
    "function, though for this assignment we will not do that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients Implementation (Look at `train_pg.py`)\n",
    "\n",
    "\n",
    "**You will need to write code in `train_pg.py`. The places where you need to write code are\n",
    " clearly indicated with the comments `START OF YOUR CODE` and\n",
    "`END OF YOUR CODE`. \n",
    "You do not need to change any other files for this part of the assignment.**\n",
    "\n",
    "The function `train_PG` is used to perform the training for policy\n",
    "gradients. The parameters passed into this function specify the algorithm’s\n",
    "hyperparameters and environment. The `Agent` class contains methods that\n",
    "define the computation graph, sample trajectories, estimate returns, and update\n",
    "the parameters of the policy.  The dataflow of the code is structured like this:\n",
    "\n",
    "- Build a static computation graph in Tensorflow.\n",
    "- Set up a Tensorflow session to initialize the parameters of the\n",
    "computation graph.  This is the only time you need to set up the session.\n",
    "\n",
    "Then we will repeat Steps 3 through 5 for $N$ iterations:\n",
    "\n",
    "- Sample trajectories by executing the Tensorflow op that samples an\n",
    "action given an observation from the environment. Collect the states, actions,\n",
    "and rewards as numpy variables.\n",
    "- Estimate returns in numpy, i.e., the estimated Q values.\n",
    "- Update parameters by executing the Tensorflow op that updates the\n",
    "parameters given what you computed in Step 4.\n",
    "\n",
    "\n",
    "## Problem 1: Agent Computation Graph\n",
    "\n",
    "Implement `Agent.build_computation_graph` by looking at any parts with\n",
    "a \"Problem 1\" header in the code. Here's what you need to do:\n",
    "\n",
    "- Define the placeholder for the advantages in `Agent.define_placeholders`. We have already done similar stuff for the\n",
    "observations and actions. The advantages correspond to $R(\\tau)$ in the policy\n",
    "gradient.\n",
    "\n",
    "- Create the symbolic operation `Agent.policy_forward_pass`: This\n",
    "outputs the parameters of a distribution $\\pi_\\theta(a|s)$.  When the\n",
    "distribution is over discrete actions these parameters will be the logits of a\n",
    "categorical distribution, and when the distribution is over continuous actions\n",
    "these parameters will be the mean and the log standard deviation of a\n",
    "multivariate Gaussian distribution. This operation will be an input to\n",
    "`Agent.sample_action` and `Agent.get_log_prob`.\n",
    "\n",
    "- Create the symbolic operation `Agent.sample_action`: This produces a Tensorflow operation named `self.sy_sampled_ac` that samples an action from $\\pi_\\theta(a|s)$. This operation will be called in `Agent.sample_trajectories`.\n",
    "\n",
    "- Create the symbolic operation `Agent.get_log_prob`: Given an\n",
    "action that the agent took in the environment, this computes the log probability\n",
    "of that action under $\\pi_\\theta(a|s)$. This will be used in the loss function.\n",
    "\n",
    "- In `Agent.build_computation_graph` implement a loss function (which uses the result from `Agent.get_log_prob`) to whose gradient is $\\nabla_\\theta J(\\theta)$. Set this as `self.loss`.\n",
    "\n",
    "\n",
    "## Problem 2: Policy Gradient Loop\n",
    "\n",
    "An RL algorithm can viewed as consisting of three parts, which are reflected in\n",
    "the training loop of `train_PG`:\n",
    "\n",
    "- `Agent.sample_trajectories`: Generate samples by running the agent's policy.\n",
    "- `Agent.estimate_return`: Estimate the return by summing together (potentially discounted) rewards from the trajectories.\n",
    "- `Agent.update_parameters`: Improve the policy by running gradient optimization updates with automatic differentiation via TensorFlow.\n",
    "\n",
    "You only need to implement the parts with the \"Problem 2\" header.\n",
    "\n",
    "- **Sample trajectories**: In `Agent.sample_trajectories`, use the Tensorflow session to call `self.sy_sampled_ac` to sample an action given an observation from the environment.\n",
    "\n",
    "- **Estimate return, part 1**: We will now implement $R(\\tau)$.  Implement `Agent.sum_of_rewards`, which will return a sample estimate of the discounted return, for both the full-trajectory and reward to go. Concretely, these are:\n",
    "\n",
    "  \\begin{equation}\n",
    "  R(\\tau_i) = \\sum_{t=1}^T \\gamma^{t-1} R(s_{it},a_{it})\n",
    "  \\end{equation}\n",
    "\n",
    "  and\n",
    "\n",
    "  \\begin{equation}\n",
    "  R(\\tau_i) = \\sum_{t'=t}^T \\gamma^{t'-t} R(s_{it},a_{it})\n",
    "  \\end{equation}\n",
    "\n",
    "  See the code comments for additional details.\n",
    "  \n",
    "- **Estimate return, part 2**: in `Agent.estimate_return`, normalize the advantages to have a mean of zero and a standard deviation of one.  This is a trick for reducing variance.\n",
    "\n",
    "- **Update parameters**: In `Agent.update_parameters` use the Tensorflow session to call the update operation `self.update_op` to update the parameters of the policy. You need to create a dictionary `feed_dict` with the inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Considerations (Code, Plotting, Logging, etc.)\n",
    "\n",
    "Note that this assignment is structured slightly different from the prior ones in that you should be testing your code by running code via your command line. The plotting, though, will still be done here for reporting purposes. See the two deliverables later for what you should be plotting, and the commands for doing so, and further details on what information gets logged for plotting. Make sure that you don't change the deliverable cells apart from adjusting your log directory names, as the graders need consistency in seeing the results.\n",
    "\n",
    "Load this cell for some initial setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure your Python version makes sense.\n",
    "import sys\n",
    "sys.version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "\n",
    "# Daniel note: tsplot() might be deprecated\n",
    "def plot_data(data, value=\"AverageReturn\"):\n",
    "    if isinstance(data, list):\n",
    "        data = pd.concat(data, ignore_index=True)\n",
    "    sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "    sns.tsplot(data=data, time=\"Iteration\", value=value, unit=\"Unit\", condition=\"Condition\")\n",
    "    plt.legend(loc='best').set_draggable(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_datasets(fpath, condition=None):\n",
    "    unit = 0\n",
    "    datasets = []\n",
    "    for root, dir, files in os.walk(fpath):\n",
    "        if 'log.txt' in files:\n",
    "            param_path = open(os.path.join(root,'params.json'))\n",
    "            params = json.load(param_path)\n",
    "            exp_name = params['exp_name']   \n",
    "            log_path = os.path.join(root,'log.txt')\n",
    "            #experiment_data = pd.read_table(log_path)\n",
    "            experiment_data = pd.read_csv(log_path, sep='\\t')\n",
    "            experiment_data.insert(\n",
    "                len(experiment_data.columns),\n",
    "                'Unit',\n",
    "                unit\n",
    "            )        \n",
    "            experiment_data.insert(\n",
    "                len(experiment_data.columns),\n",
    "                'Condition',\n",
    "                condition or exp_name\n",
    "            )\n",
    "            datasets.append(experiment_data)\n",
    "            unit += 1\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def plot(logdir, head, legend, value='AverageReturn'):\n",
    "    use_legend = False\n",
    "    if legend is not None:\n",
    "        assert len(legend) == len(logdir), \"Must give a legend title for each set of experiments.\"\n",
    "        use_legend = True\n",
    "\n",
    "    data = []\n",
    "    if use_legend:\n",
    "        for logdir, legend_title in zip(logdir, legend):\n",
    "            data += get_datasets(os.path.join(head,logdir), legend_title)\n",
    "    else:\n",
    "        for logdir in logdir:\n",
    "            data += get_datasets(os.path.join(head,logdir))\n",
    "\n",
    "    if isinstance(value, list):\n",
    "        values = value\n",
    "    else:\n",
    "        values = [value]\n",
    "    for value in values:\n",
    "        plot_data(data, value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 1 of 2: Policy Gradients on CartPole-v0\n",
    "\n",
    "\n",
    "## Commands to Run\n",
    "\n",
    "Once you think you have finished implementing the above, then run these commands:\n",
    "\n",
    "```\n",
    "python train_pg.py CartPole-v0 -n 100 -b 1000 -e 3      -dna --exp_name sb_no_rtg_dna\n",
    "python train_pg.py CartPole-v0 -n 100 -b 1000 -e 3 -rtg -dna --exp_name sb_rtg_dna\n",
    "python train_pg.py CartPole-v0 -n 100 -b 1000 -e 3 -rtg      --exp_name sb_rtg_na\n",
    "python train_pg.py CartPole-v0 -n 100 -b 5000 -e 3      -dna --exp_name lb_no_rtg_dna\n",
    "python train_pg.py CartPole-v0 -n 100 -b 5000 -e 3 -rtg -dna --exp_name lb_rtg_dna\n",
    "python train_pg.py CartPole-v0 -n 100 -b 5000 -e 3 -rtg      --exp_name lb_rtg_na\n",
    "```\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "- `-n`: Number of iterations.\n",
    "\n",
    "- `-b`: Batch size (number of state-action pairs sampled while\n",
    "acting according to the current policy at each iteration).\n",
    "\n",
    "- `-e`: Number of experiments to run with the same configuration.\n",
    "Each experiment will start with a different randomly initialized policy, and\n",
    "have a different stream of random numbers. These will be run in parallel and\n",
    "saved under the same higher-level directory, and plotted together.\n",
    "\n",
    "- `-dna`: Flag: if present, sets `normalize_advantages=False`. Otherwise, by default it's True.\n",
    "\n",
    "- `-rtg`: Flag: if present, sets `reward_to_go=True`. Otherwise, by default it's False.\n",
    "\n",
    "- `--exp_name`: Name for experiment, which goes into the name for the data directory.\n",
    "\n",
    "This will let us investigate the impact of different hyperparameters on \n",
    "performance (i.e., the average episode reward).\n",
    "\n",
    "After the above is finished, your directory structure should look something this where the date will vary based on when you started the calls above:\n",
    "\n",
    "```\n",
    "data_pg/\n",
    "    lb_no_rtg_dna_CartPole-v0_01-04-2019_16-57-36\n",
    "    lb_rtg_dna_CartPole-v0_01-04-2019_17-04-55\n",
    "    lb_rtg_na_CartPole-v0_01-04-2019_17-12-14\n",
    "    sb_no_rtg_dna_CartPole-v0_01-04-2019_16-49-36\n",
    "    sb_rtg_dna_CartPole-v0_01-04-2019_16-52-43\n",
    "    sb_rtg_na_CartPole-v0_01-04-2019_16-55-20\n",
    "```\n",
    "\n",
    "where each of the six sub-directories under `data_pg` has `1`, `11`, and `21` subdirectories.\n",
    "\n",
    "## Plot Results and Answer our Questions\n",
    "\n",
    "- Graph the results of your experiments using the commands we provide.\n",
    "   - In the first graph, compare the learning curves (average return at each iteration) for the experiments prefixed with `sb_`. (The small batch experiments.)\n",
    "   - In the second graph, compare the learning curves for the experiments prefixed with `lb_`. (The large batch experiments.)\n",
    "   \n",
    "All you need to do is to **change the file names in `sb_experiments` and `lb_experiments`**.\n",
    "\n",
    "\n",
    "Answer the following questions briefly in the designated cell with \"Short-Answer Questions\" as the title.\n",
    "\n",
    "- Which gradient estimator has better performance without advantage-centering— the trajectory-centric one, or the one using reward-to-go?\n",
    "- Did advantage centering help?\n",
    "- Did the batch size make an impact?\n",
    "\n",
    "You may wish to adjust other hyperparameters that were not listed, such as the\n",
    "number of layers and units per layer, to better understand their effect, but\n",
    "it is not required. If you report stuff like this, **be sure to write the exact command you used**.\n",
    "\n",
    "You should expect to see the best configurations attain a score of 200 for most\n",
    "iterations, though performance may decrease on occasion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust files in these two lists! The following is an example from my system where\n",
    "# I had these as my directory names.\n",
    "\n",
    "lb_experiments = [\n",
    "    #'lb_no_rtg_dna_CartPole-v0_01-04-2019_20-49-49',\n",
    "    #'lb_rtg_dna_CartPole-v0_01-04-2019_20-52-02',\n",
    "    #'lb_rtg_na_CartPole-v0_01-04-2019_20-54-11',\n",
    "]\n",
    "\n",
    "sb_experiments = [\n",
    "    #'sb_no_rtg_dna_CartPole-v0_01-04-2019_20-44-52',\n",
    "    #'sb_rtg_dna_CartPole-v0_01-04-2019_20-48-21',\n",
    "    #'sb_rtg_na_CartPole-v0_01-04-2019_20-48-57',\n",
    "]\n",
    "    \n",
    "\n",
    "# DON'T CHANGE THE FOLLOWING CODE !! If you want to do more experimentation than\n",
    "# what we have here, that's fine, but do that in the cells at the end of the notebook.\n",
    "print(\"Results with the small batch:\")\n",
    "plot(sb_experiments, head='data_pg', legend=None, value='AverageReturn')\n",
    "print(\"\\nResults with the large batch:\")\n",
    "plot(lb_experiments, head='data_pg', legend=None, value='AverageReturn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-Answer Questions\n",
    "\n",
    "Write your answers to the questions we asked in the deliverable above.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2 of 2: Policy Gradients on Pendulum-v0\n",
    "\n",
    "In this subsection, you will run another classic control problem, Pendulum-v0,\n",
    "which is continuous control with action dimension one. Pendulum is a relatively\n",
    "harder and unstable task compared to CartPole, and it is difficult to\n",
    "consistently get above -1000 with vanilla policy gradients.  Thus, in this part\n",
    "of the assignment we will do some hyperparameter tuning. Tune the batch size and\n",
    "learning rate parameters by running the following command:\n",
    "\n",
    "```\n",
    "python train_pg.py Pendulum-v0 -ep 1000 --discount 0.99 -n 400 -e 3 -rtg \\\n",
    "        -b <b*> -lr <l*> --exp_name pend_b_<*b>_lr_<l*>\n",
    "```\n",
    "\n",
    "while varying the values for the batch size `b*` and the learning rate `l*`.\n",
    "\n",
    "Report two plots in your writeup, corresponding to the best two hyperparameter\n",
    "combinations that you tried. Please use the same code we provided earlier. *Each plot\n",
    "should be the average of AT LEAST three random seeds.*\n",
    "\n",
    "In addition, report all values of the two hyperparameters you tried, and comment\n",
    "on the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put your answers and two plots here!\n",
    "# It's probably easiest to use the same plotting code that we did earlier but if you *really*\n",
    "# need to modify it, it might be best just to rewrite the plotting script in this ceoo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anything Else\n",
    "\n",
    "If you'd like to try out more extensions or other things, feel free to put your comments and results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
