{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Deep Q Networks\n",
    "\n",
    "**Before doing this, be sure you review the `README.md` file from the homework!**\n",
    "\n",
    "**HUGE NOTE**: the third deliverable asks you to run Pong for several thousand steps.\n",
    "Please start early, as the evaluation portion of this assignment can take many hours.\n",
    "We provide some timing benchmarks for machines in the third deliverable.\n",
    "\n",
    "In this part of the assignment, you will implement the Deep Q-Network algorithm\n",
    "(DQN) [1], along with the Double DQN extension [2]. At a high level, the `dqn.py`\n",
    "code builds a TensorFlow computational graph in the class initialization method.\n",
    "Then, to train, it iterates through environment steps and model updates.\n",
    "\n",
    "## Review\n",
    "\n",
    "Recall from lecture that the DQN algorithm performs the following optimization:\n",
    "\n",
    "\\begin{equation}\\label{eq:dqn}\n",
    "{\\rm minimize}_{\\theta} \\;\\; \\mathbb{E}_{(s_t,a_t,r_t,s_{t+1})\\sim D}\n",
    "\\left[ \n",
    "\\Big(r_t + \\gamma \\max_{a \\in \\mathcal{A}} Q_{\\theta^-}(s_{t+1},a) - Q_\\theta(s_t,a_t)\\Big)^2\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Here, $(s_t,a_t,r_t,s_{t+1})$ are batches of samples from the experience replay\n",
    "buffer $D$, which is designed to store the past $N$ samples (where usually\n",
    "$N=1,000,000$ for Atari benchmarks) so as to break correlation among the\n",
    "training data used for updating $\\theta$. In addition, we use $\\theta$ to\n",
    "represent the *current* or *online* network, whereas $\\theta^-$\n",
    "represents the *target* network. Both networks use the same architecture,\n",
    "and we use $Q_\\theta(s,a)$ or $Q_{\\theta^-}(s,a)$ to denote which of the two\n",
    "parameters is being applied to evaluate the state-action tuple $(s,a)$.\n",
    "\n",
    "The target network starts off by getting matched to the current network, but\n",
    "remains frozen (usually for thousands of steps) before getting updated again to\n",
    "match the network. The process repeats throughout training, with the goal of\n",
    "increasing the stability of the targets $r_t + \\gamma \\max_{a \\in \\mathcal{A}}\n",
    "Q_{\\theta^-}(s_{t+1},a)$. For more details, we recommend reading [1].\n",
    "\n",
    "In *Double* DQN, the target value changes slightly. The new optimization\n",
    "problem is:\n",
    "\n",
    "\\begin{equation}\\label{eq:ddqn}\n",
    "{\\rm minimize}_{\\theta} \\;\\; \\mathbb{E}_{(s_t,a_t,r_t,s_{t+1})\\sim D}\n",
    "\\left[ \n",
    "\\Big(r_t + \\gamma Q_{\\theta^-} \\Big( s_{t+1}, {\\rm argmax}_{a\\in \\mathcal{A}} Q_\\theta(s_{t+1},a) \\Big)  - Q_\\theta(s_t,a_t)\\Big)^2\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Notice the key difference in the target terms, which we denote $Y_t^{\\rm DQN}$\n",
    "and $Y_t^{\\rm DoubleDQN}$ following the notation in [2]. Explicitly, we\n",
    "have\n",
    "\n",
    "\\begin{equation}\\label{eq:dqn_targ}\n",
    "Y_t^{\\rm DQN} = r_t + \\gamma \\max_{a \\in \\mathcal{A}} Q_{\\theta^-}(s_{t+1},a)\n",
    "\\end{equation}\n",
    "\n",
    "in DQN, and\n",
    "\n",
    "\\begin{equation}\\label{eq:ddqn_targ}\n",
    "Y_t^{\\rm DoubleDQN} = r_t + \\gamma\n",
    "Q_{\\theta^-} \\Big( s_{t+1}, {\\rm argmax}_{a\\in \\mathcal{A}} Q_\\theta(s_{t+1},a) \\Big)\n",
    "\\end{equation}\n",
    "\n",
    "in DDQN. Intuitively, DDQN helps to mitigate the issue of over-optimistic values\n",
    "from DQN. In DQN, for a given state $s$, the Q-network takes the maximum over\n",
    "quantities $Q_{\\theta^-}(s,a_k)$ for all actions $a_k \\in \\mathcal{A}$. In DDQN,\n",
    "we consider the same $Q_{\\theta^-}(s,a_k)$ values, yet are not guaranteed to\n",
    "take the maximum one because the action selection is done by a different\n",
    "network.  For more details, we recommend reading [2]. The paper also\n",
    "covers the Double Q-Learning algorithm, which was originally developed in the\n",
    "tabular settings.  The Double *Deep Q-Network* algorithm, which you will\n",
    "also implement, is the minimal extension of DQN towards Double Q-Learning.\n",
    "\n",
    "[1]: [DQN](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf), read the Nature 2015 paper, not the NIPS 2013 workshop paper\n",
    "\n",
    "[2]: [DDQN](https://arxiv.org/abs/1509.06461) at AAAI, 2016.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN and DDQN Implementation (look at `dqn.py`)\n",
    "\n",
    "\n",
    "**You will need to write code in `dqn.py` file in three specific spots.\n",
    "These are clearly indicated with the comments `START OF YOUR CODE` and\n",
    "`END OF YOUR CODE`.** Put your code between those comments. Concretely, they correspond to:\n",
    "\n",
    "- Computing the Bellman Error.\n",
    "- Environment Stepping\n",
    "- Training\n",
    "\n",
    "**See the comments in `dqn.py`, which provide detailed instructions. You\n",
    "should not need to modify any other files for this part of the assigment**,\n",
    "but you should browse through `dqn_utils.py`\n",
    "to understand how supporting code (such as the replay buffer) is implemented,\n",
    "and `train_dqn.py` to understand the command line arguments and algorithm\n",
    "hyperparameters. *Look at the three \"deliverable\" cells \n",
    "later for the commands you can use on your command line to test your code*.\n",
    "\n",
    "Some advice: implementing DQN can be tricky, and it is sometimes difficult to know if your\n",
    "algorithm is working immediately due to the many samples necessary for deep\n",
    "reinforcement learning. Thus, here is some advice:\n",
    "\n",
    "- For implementing the computational graph, print out the tensors that you\n",
    "form and check that the shapes make sense. When printed, the shape of the\n",
    "leading dimension for certain tensors might be `?`, which represents a\n",
    "\"flexible\" batch size: it either corresponds to the training batch size\n",
    "hyperparameter, or just one.\n",
    "\n",
    "- Do not engage in (extensive) hyperparmeter tuning. We provide\n",
    "hyperparameters for you in `train_dqn.py` for both CartPole and Pong that\n",
    "have empirically worked for our solutions. If you have the time, you may find it\n",
    "helpful to tune the learning rate for CartPole.\n",
    "\n",
    "- Run with CartPole as a sanity check before testing with Pong. If you're\n",
    "getting at least 100 points on CartPole, that's a good sign. DQN, has high\n",
    "variance, so run several random seeds.\n",
    "\n",
    "- Once DQN is implemented, the addition of DDQN should only require on the\n",
    "order of 2-4 additional lines of code, excluding `if/else` branches to account\n",
    "for `args.double_q`.\n",
    "\n",
    "For reference our DQN solution on PongNoFrameskip-v4 gets results in the following\n",
    "figure, where we ran for five random seeds.\n",
    "\n",
    "<img src=\"files/pong.png\" width=\"500\">\n",
    "\n",
    "Each of the above curves was generated by running:\n",
    "\n",
    "```\n",
    "python train_dqn.py PongNoFrameskip-v4 --num_steps 4000000\n",
    "```\n",
    "\n",
    "which trains Pong for 4 million steps. The *plot* itself was generated by the same plotting code that you will use later on in this assignment (see \"Deliverable\" cells).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Considerations (Code, Plotting, Logging, etc.)\n",
    "\n",
    "As in the first part on policy gradients, you should test your code by running commands on your command line (rather than by running Jupyter notebook cells). The plotting, though, will still be done here for reporting purposes. See the three deliverables later for what you should be plotting, and the commands for doing so. Make sure that you don't change the cells, as the graders need consistency in seeing the results.\n",
    "\n",
    "When you run the DQN, your log directory by default should be `data_dqn`, and its structure might look like this, where the date will vary based on when you ran the code:\n",
    "\n",
    "```\n",
    "data_dqn/\n",
    "    dqn_PongNoFrameskip-v4_20-01-2019_11-58-27/\n",
    "        log.txt\n",
    "    dqn_PongNoFrameskip-v4_20-01-2019_12-00-04/\n",
    "        log.txt\n",
    "```\n",
    "\n",
    "Each of the sub-files contains the algorithm used, the environment, and the date when the trial was launched.\n",
    "The 'log.txt' files store the statistics from training, and are needed for the plots.\n",
    "\n",
    "Load the following cell, which will get the plotting and other supporting code set up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure your Python version makes sense\n",
    "import sys\n",
    "sys.version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "\n",
    "def plot(logdir, env, double_q, trials):\n",
    "    fig = plt.figure(figsize=(9,7))\n",
    "\n",
    "    for path in trials:\n",
    "        full_path = os.path.join(logdir, path)\n",
    "        log_path = os.path.join(full_path,'log.txt')\n",
    "        name = path.replace('NoFrameskip-v4','')\n",
    "        #data = pd.read_table(log_path)  # Daniel: this got deprecated\n",
    "        data = pd.read_csv(log_path, sep='\\t')  # in favor of this\n",
    "        steps_million = data['Steps'] / 1e6\n",
    "        plt.plot(steps_million, data['Avg_Last_100_Episodes'], lw=2, label=name)\n",
    "\n",
    "    # Bells and whistles\n",
    "    plt.tick_params(axis='x', labelsize=18)\n",
    "    plt.tick_params(axis='y', labelsize=18)\n",
    "    plt.legend(loc='best', fontsize=20)\n",
    "    plt.xlabel('Training Steps (in Millions)', fontsize=20)\n",
    "    plt.ylabel('Avg Last 100 Episodes', fontsize=20)\n",
    "    if double_q:\n",
    "        plt.title('{} Double DQN'.format(env), fontsize=24)\n",
    "    else:\n",
    "        plt.title('{} DQN'.format(env), fontsize=24)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 1 of 3: DQN on CartPole-v0\n",
    "\n",
    "On your command line, run:\n",
    "\n",
    "```\n",
    "python train_dqn.py CartPole-v0 --num_steps 100000\n",
    "```\n",
    "\n",
    "*Do this five times*.\n",
    "\n",
    "Save the strings (starting with `'dqn_CartPole-v0_`) and put them in the `trials` list for plotting. Thus your data directory might look like this (with the dates obviously different):\n",
    "\n",
    "```\n",
    "data_dqn/\n",
    "    dqn_CartPole-v0_01-04-2019_14-39-27\n",
    "    dqn_CartPole-v0_01-04-2019_15-04-41\n",
    "    dqn_CartPole-v0_01-04-2019_15-06-31\n",
    "    dqn_CartPole-v0_01-04-2019_15-08-40\n",
    "    dqn_CartPole-v0_01-04-2019_15-10-05\n",
    "```\n",
    "\n",
    "**Adjust the `trials` list below with your data directory names**. Run the following plotting code and save the output for your homework submission.\n",
    "\n",
    "You should expect to see performance near 200 points for most of the curves, though DQN can be brittle on Pong and often performance will decline unexpectedly. One possibility might be to reduce the learning rate after a certain amount of steps. Feel free to try this out at the end of this assignemnt. See the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put a list of your DQN with CartPole trials here. Here's an example where I've put in\n",
    "# my own trials, but you'll need to put in yours. The `trials` list should be length 5.\n",
    "\n",
    "trials = [\n",
    "    #'dqn_CartPole-v0_01-04-2019_21-00-10',\n",
    "    #'dqn_CartPole-v0_01-04-2019_21-02-28',\n",
    "    #'dqn_CartPole-v0_01-04-2019_21-04-05',\n",
    "    #'dqn_CartPole-v0_01-04-2019_21-05-44',\n",
    "    #'dqn_CartPole-v0_01-04-2019_21-08-36',\n",
    "]\n",
    "\n",
    "# DON'T CHANGE THE FOLLOWING CODE !! If you want to do more experimentation than\n",
    "# what we have here, that's fine, but do that in the cells at the end of the notebook.\n",
    "if len(trials) != 5:\n",
    "    print(\"WARNING! len(trials) = {}\".format(len(trials)))\n",
    "    print(\"Please include 5 results for your final report.\")\n",
    "for t in trials:\n",
    "    assert t[:4] == 'dqn_'\n",
    "    assert t[:7] != 'double-'\n",
    "    assert 'CartPole-v0' in t\n",
    "    assert 'Pong' not in t\n",
    "plot(logdir='data_dqn', env='CartPole-v0', double_q=False, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2 of 3: DDQN on CartPole-v0\n",
    "\n",
    "On your command line, run:\n",
    "\n",
    "```\n",
    "python train_dqn.py CartPole-v0 --num_steps 100000  --double_q\n",
    "```\n",
    "\n",
    "For *five* times.\n",
    "\n",
    "**Adjust the `trials` list below with your data directory names**. Run the following plotting code and save the output for your homework submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, adjust the list below based on the data you have...\n",
    "\n",
    "trials = [\n",
    "    #'double-dqn_CartPole-v0_01-04-2019_21-10-04',\n",
    "    #'double-dqn_CartPole-v0_01-04-2019_21-13-31',\n",
    "    #'double-dqn_CartPole-v0_01-04-2019_21-14-44',\n",
    "    #'double-dqn_CartPole-v0_01-04-2019_21-15-56',\n",
    "    #'double-dqn_CartPole-v0_01-04-2019_21-17-35',\n",
    "]\n",
    "\n",
    "# DON'T CHANGE THE FOLLOWING CODE !! If you want to do more experimentation than\n",
    "# what we have here, that's fine, but do that in the cells at the end of the notebook.\n",
    "if len(trials) != 5:\n",
    "    print(\"WARNING! len(trials) = {}\".format(len(trials)))\n",
    "    print(\"Please include 5 results for your final report.\")\n",
    "for t in trials:\n",
    "    assert t[:4] != 'dqn_'\n",
    "    assert t[:7] == 'double-'\n",
    "    assert 'CartPole-v0' in t\n",
    "    assert 'Pong' not in t\n",
    "plot(logdir='data_dqn', env='CartPole-v0', double_q=True, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 3 of 3: DQN on PongNoFrameskip-v4\n",
    "\n",
    "On your command line, run:\n",
    "\n",
    "```\n",
    "python train_dqn.py PongNoFrameskip-v4 --num_steps 600000\n",
    "```\n",
    "\n",
    "For *one* time. Due to computational limitations, we only ask for 600k steps; this should be enough to see some improvement in your results from random performace (which is around -20 to -21). The GPU is the major bottleneck for code speed. Here are some timing benchmarks we have:\n",
    "\n",
    "- On a two year old Macbook Pro laptop (10.13.6, High Sierra) with no GPU, 600k steps completes in **7.2 hours**.\n",
    "- On a machine with: NVIDIA Titan X (Pascal) GPU, an i7 7700 CPU with 3.60 GHz and 4 cores, and 64 GB of RAM, 600k steps using the GPU completes in **0.46 hours**.\n",
    "- If you do the same as the machine above, but with a CPU (that is, the same i7 7700 CPU, etc., but using `tensorflow` instead of `tensorflow-gpu`, so ignoring the GPU), 600k steps completes in about the same time as the Macbook Pro laptop with the CPU, about 7 hours. Thus, a good GPU is essential for pure code speed.\n",
    "\n",
    "Do not run this until the CartPole results show some improvement in reward. Feel free to run longer if you wish to double check your implementation, but it is not required.\n",
    "\n",
    "**Adjust the `trials` list below with your data directory name**. Run the following plotting code and save the output for your homework submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual ...\n",
    "trials = [\n",
    "    #'dqn_PongNoFrameskip-v4_01-04-2019_21-20-48'\n",
    "]\n",
    "\n",
    "# DON'T CHANGE THE FOLLOWING CODE !! If you want to do more experimentation than\n",
    "# what we have here, that's fine, but do that in the cells at the end of the notebook.\n",
    "if len(trials) != 1:\n",
    "    print(\"WARNING! len(trials) = {}\".format(len(trials)))\n",
    "    print(\"Please include 1 result for Pong for your final report.\")\n",
    "for t in trials:\n",
    "    assert t[:4] == 'dqn_'\n",
    "    assert t[:7] != 'double-'\n",
    "    assert 'CartPole-v0' not in t\n",
    "    assert 'PongNoFrameskip-v4' in t\n",
    "plot(logdir='data_dqn', env='PongNoFrameskip-v4', double_q=False, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anything Else\n",
    "\n",
    "If you'd like to try out more extensions or other things, feel free to put your comments and results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
